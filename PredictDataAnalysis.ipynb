{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load data\n",
    "alt_data = pd.read_csv('bigdatatest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=alt_data)\n",
    "df=df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['altmetric_id'], axis=1)\n",
    "df=df.drop(['pub_year','followers','posts_count','unique_users_count','socialmedia_posts_counts'], axis=1)\n",
    "#data_trained.columns=['Rank','H index,Total Docs. (2017)','Total Docs. (3years),Total Refs.','Total Cites (3years)','Cites / Doc. (2years)','Ref. / Doc.',\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>altmetric_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438696</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438697</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438698</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438699</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438700</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438701</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438702</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438703</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438704</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438705</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438706</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438707</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438708</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438709</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438710</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438711</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438712</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438713</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438714</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438715</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438716</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438717</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438718</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438719</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438720</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438721</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438722</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438723</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438724</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438725</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438726 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        altmetric_score\n",
       "0                   3.0\n",
       "1                   0.0\n",
       "2                   1.0\n",
       "3                   3.0\n",
       "4                   0.0\n",
       "5                   2.0\n",
       "6                   0.0\n",
       "7                   2.0\n",
       "8                   1.0\n",
       "9                   1.0\n",
       "10                  2.0\n",
       "11                  0.0\n",
       "12                  1.0\n",
       "13                  2.0\n",
       "14                  2.0\n",
       "15                  3.0\n",
       "16                  3.0\n",
       "17                  1.0\n",
       "18                  3.0\n",
       "19                  0.0\n",
       "20                  2.0\n",
       "21                  1.0\n",
       "22                  1.0\n",
       "23                  0.0\n",
       "24                  2.0\n",
       "25                  0.0\n",
       "26                  1.0\n",
       "27                  1.0\n",
       "28                  1.0\n",
       "29                  0.0\n",
       "...                 ...\n",
       "438696              2.0\n",
       "438697              2.0\n",
       "438698              1.0\n",
       "438699              3.0\n",
       "438700              1.0\n",
       "438701              3.0\n",
       "438702              3.0\n",
       "438703              3.0\n",
       "438704              2.0\n",
       "438705              1.0\n",
       "438706              3.0\n",
       "438707              3.0\n",
       "438708              2.0\n",
       "438709              1.0\n",
       "438710              2.0\n",
       "438711              1.0\n",
       "438712              3.0\n",
       "438713              3.0\n",
       "438714              3.0\n",
       "438715              1.0\n",
       "438716              3.0\n",
       "438717              0.0\n",
       "438718              3.0\n",
       "438719              3.0\n",
       "438720              3.0\n",
       "438721              1.0\n",
       "438722              0.0\n",
       "438723              2.0\n",
       "438724              3.0\n",
       "438725              0.0\n",
       "\n",
       "[438726 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "target=pd.DataFrame(data=df['altmetric_score'])\n",
    "est = preprocessing.KBinsDiscretizer(n_bins=[4], encode='ordinal').fit(target)\n",
    "new_target=pd.DataFrame(data=est.transform(target))\n",
    "new_target.columns=['altmetric_score']\n",
    "new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.drop(['altmetric_score'], axis=1)\n",
    "target=new_target['altmetric_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation:\n",
      "0.7144468492091411\n"
     ]
    }
   ],
   "source": [
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)\n",
    "my_imputer = Imputer()\n",
    "data_trained = my_imputer.fit_transform(X_train)\n",
    "data_tested = my_imputer.transform(X_test)\n",
    "print(\"Mean Absolute Error from Imputation:\")\n",
    "print(score_dataset(data_trained, data_tested, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.102517</td>\n",
       "      <td>0.229087</td>\n",
       "      <td>0.851452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.049130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.404770</td>\n",
       "      <td>0.020913</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.045542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.075651</td>\n",
       "      <td>0.056084</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>0.006883</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.016541</td>\n",
       "      <td>0.088174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.243606</td>\n",
       "      <td>0.075095</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>0.004971</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.005028</td>\n",
       "      <td>0.038024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.331749</td>\n",
       "      <td>0.027510</td>\n",
       "      <td>0.026254</td>\n",
       "      <td>0.008638</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.062996</td>\n",
       "      <td>0.016037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005356</td>\n",
       "      <td>0.188213</td>\n",
       "      <td>0.179925</td>\n",
       "      <td>0.115795</td>\n",
       "      <td>0.193675</td>\n",
       "      <td>0.478692</td>\n",
       "      <td>0.060985</td>\n",
       "      <td>0.054995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.965525</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.053409</td>\n",
       "      <td>0.040875</td>\n",
       "      <td>0.021348</td>\n",
       "      <td>0.005889</td>\n",
       "      <td>0.029946</td>\n",
       "      <td>0.007786</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.071665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.099883</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.017295</td>\n",
       "      <td>0.062857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.214486</td>\n",
       "      <td>0.035171</td>\n",
       "      <td>0.011197</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.006285</td>\n",
       "      <td>0.013233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.039830</td>\n",
       "      <td>0.217681</td>\n",
       "      <td>0.065977</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.055066</td>\n",
       "      <td>0.114603</td>\n",
       "      <td>0.030920</td>\n",
       "      <td>0.042642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.059643</td>\n",
       "      <td>0.148289</td>\n",
       "      <td>0.026262</td>\n",
       "      <td>0.021331</td>\n",
       "      <td>0.025223</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.013977</td>\n",
       "      <td>0.049066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.041147</td>\n",
       "      <td>0.141635</td>\n",
       "      <td>0.026946</td>\n",
       "      <td>0.026690</td>\n",
       "      <td>0.013764</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.019507</td>\n",
       "      <td>0.026090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.981387</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.378987</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.018120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.441030</td>\n",
       "      <td>0.015209</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.020183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.053556</td>\n",
       "      <td>0.054183</td>\n",
       "      <td>0.025255</td>\n",
       "      <td>0.022874</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>0.025023</td>\n",
       "      <td>0.015535</td>\n",
       "      <td>0.051461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.102517</td>\n",
       "      <td>0.229087</td>\n",
       "      <td>0.851452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.049130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.068715</td>\n",
       "      <td>0.064639</td>\n",
       "      <td>0.004390</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.037777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.118993</td>\n",
       "      <td>0.107414</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.017748</td>\n",
       "      <td>0.024758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.030963</td>\n",
       "      <td>0.088403</td>\n",
       "      <td>0.012285</td>\n",
       "      <td>0.009234</td>\n",
       "      <td>0.014295</td>\n",
       "      <td>0.011787</td>\n",
       "      <td>0.023982</td>\n",
       "      <td>0.059441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.069271</td>\n",
       "      <td>0.167300</td>\n",
       "      <td>0.023523</td>\n",
       "      <td>0.020683</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.016969</td>\n",
       "      <td>0.011011</td>\n",
       "      <td>0.039979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.059438</td>\n",
       "      <td>0.147338</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>0.014994</td>\n",
       "      <td>0.006292</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>0.023609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.049810</td>\n",
       "      <td>0.081749</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.028375</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.047312</td>\n",
       "      <td>0.024485</td>\n",
       "      <td>0.058088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.010975</td>\n",
       "      <td>0.093156</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.022174</td>\n",
       "      <td>0.044646</td>\n",
       "      <td>0.053136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.047529</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.010005</td>\n",
       "      <td>0.018571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.171964</td>\n",
       "      <td>0.081749</td>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>0.015615</td>\n",
       "      <td>0.015083</td>\n",
       "      <td>0.012267</td>\n",
       "      <td>0.039066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.052795</td>\n",
       "      <td>0.134981</td>\n",
       "      <td>0.018528</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>0.052718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.095815</td>\n",
       "      <td>0.106464</td>\n",
       "      <td>0.030531</td>\n",
       "      <td>0.024994</td>\n",
       "      <td>0.029340</td>\n",
       "      <td>0.019889</td>\n",
       "      <td>0.011413</td>\n",
       "      <td>0.049098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.098186</td>\n",
       "      <td>0.095057</td>\n",
       "      <td>0.009747</td>\n",
       "      <td>0.006749</td>\n",
       "      <td>0.007255</td>\n",
       "      <td>0.005505</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.038024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131588</th>\n",
       "      <td>0.040942</td>\n",
       "      <td>0.046578</td>\n",
       "      <td>0.012768</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>0.017395</td>\n",
       "      <td>0.027401</td>\n",
       "      <td>0.071944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131589</th>\n",
       "      <td>0.149312</td>\n",
       "      <td>0.075095</td>\n",
       "      <td>0.014138</td>\n",
       "      <td>0.013569</td>\n",
       "      <td>0.006550</td>\n",
       "      <td>0.006057</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>0.023663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131590</th>\n",
       "      <td>0.004243</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.045475</td>\n",
       "      <td>0.042238</td>\n",
       "      <td>0.042299</td>\n",
       "      <td>0.184266</td>\n",
       "      <td>0.060633</td>\n",
       "      <td>0.047519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131591</th>\n",
       "      <td>0.550776</td>\n",
       "      <td>0.024715</td>\n",
       "      <td>0.006807</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.011944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131592</th>\n",
       "      <td>0.016242</td>\n",
       "      <td>0.188213</td>\n",
       "      <td>0.021589</td>\n",
       "      <td>0.019140</td>\n",
       "      <td>0.007989</td>\n",
       "      <td>0.030319</td>\n",
       "      <td>0.036099</td>\n",
       "      <td>0.018904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131593</th>\n",
       "      <td>0.088089</td>\n",
       "      <td>0.056084</td>\n",
       "      <td>0.008781</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.039721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131594</th>\n",
       "      <td>0.026251</td>\n",
       "      <td>0.303232</td>\n",
       "      <td>0.061385</td>\n",
       "      <td>0.060707</td>\n",
       "      <td>0.070317</td>\n",
       "      <td>0.133423</td>\n",
       "      <td>0.033233</td>\n",
       "      <td>0.058518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131595</th>\n",
       "      <td>0.149576</td>\n",
       "      <td>0.060837</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>0.065789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131596</th>\n",
       "      <td>0.225724</td>\n",
       "      <td>0.032319</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.005480</td>\n",
       "      <td>0.039313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131597</th>\n",
       "      <td>0.443518</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.025027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131598</th>\n",
       "      <td>0.082997</td>\n",
       "      <td>0.097909</td>\n",
       "      <td>0.008136</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>0.018552</td>\n",
       "      <td>0.045166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131599</th>\n",
       "      <td>0.234387</td>\n",
       "      <td>0.046578</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.005480</td>\n",
       "      <td>0.022417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131600</th>\n",
       "      <td>0.114194</td>\n",
       "      <td>0.225285</td>\n",
       "      <td>0.084666</td>\n",
       "      <td>0.064358</td>\n",
       "      <td>0.053252</td>\n",
       "      <td>0.056029</td>\n",
       "      <td>0.013022</td>\n",
       "      <td>0.032127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131601</th>\n",
       "      <td>0.223910</td>\n",
       "      <td>0.058935</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.006385</td>\n",
       "      <td>0.051375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131602</th>\n",
       "      <td>0.093620</td>\n",
       "      <td>0.062738</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.012368</td>\n",
       "      <td>0.044006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131603</th>\n",
       "      <td>0.200029</td>\n",
       "      <td>0.026616</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.021923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131604</th>\n",
       "      <td>0.392918</td>\n",
       "      <td>0.035171</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.005380</td>\n",
       "      <td>0.022481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131605</th>\n",
       "      <td>0.226807</td>\n",
       "      <td>0.038023</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.023298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131606</th>\n",
       "      <td>0.102517</td>\n",
       "      <td>0.229087</td>\n",
       "      <td>0.851452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.049130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131607</th>\n",
       "      <td>0.040503</td>\n",
       "      <td>0.157795</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.004925</td>\n",
       "      <td>0.017949</td>\n",
       "      <td>0.055832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131608</th>\n",
       "      <td>0.337372</td>\n",
       "      <td>0.043726</td>\n",
       "      <td>0.014380</td>\n",
       "      <td>0.008810</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.021117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131609</th>\n",
       "      <td>0.217091</td>\n",
       "      <td>0.032319</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.008698</td>\n",
       "      <td>0.049678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131610</th>\n",
       "      <td>0.032251</td>\n",
       "      <td>0.047529</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.012367</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.015588</td>\n",
       "      <td>0.021217</td>\n",
       "      <td>0.067240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131611</th>\n",
       "      <td>0.032572</td>\n",
       "      <td>0.230989</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>0.063910</td>\n",
       "      <td>0.099382</td>\n",
       "      <td>0.080050</td>\n",
       "      <td>0.019708</td>\n",
       "      <td>0.069603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131612</th>\n",
       "      <td>0.013872</td>\n",
       "      <td>0.067490</td>\n",
       "      <td>0.010191</td>\n",
       "      <td>0.010389</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.028457</td>\n",
       "      <td>0.020784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131613</th>\n",
       "      <td>0.113813</td>\n",
       "      <td>0.083650</td>\n",
       "      <td>0.023120</td>\n",
       "      <td>0.022850</td>\n",
       "      <td>0.031567</td>\n",
       "      <td>0.021550</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>0.069753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131614</th>\n",
       "      <td>0.115862</td>\n",
       "      <td>0.074144</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.008999</td>\n",
       "      <td>0.040687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131615</th>\n",
       "      <td>0.010653</td>\n",
       "      <td>0.268061</td>\n",
       "      <td>0.033552</td>\n",
       "      <td>0.028963</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.050867</td>\n",
       "      <td>0.027501</td>\n",
       "      <td>0.037905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131616</th>\n",
       "      <td>0.225110</td>\n",
       "      <td>0.024715</td>\n",
       "      <td>0.007572</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.031418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131617</th>\n",
       "      <td>0.082587</td>\n",
       "      <td>0.129278</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.006125</td>\n",
       "      <td>0.008688</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.010960</td>\n",
       "      <td>0.067605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131618 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.102517  0.229087  0.851452  1.000000  0.818792  1.000000  0.014379   \n",
       "1       0.404770  0.020913  0.000403  0.000542  0.000359  0.000107  0.002262   \n",
       "2       0.075651  0.056084  0.003988  0.003486  0.006883  0.003832  0.016541   \n",
       "3       0.243606  0.075095  0.006163  0.004971  0.004587  0.001934  0.005028   \n",
       "4       0.004185  0.331749  0.027510  0.026254  0.008638  0.034707  0.062996   \n",
       "5       0.005356  0.188213  0.179925  0.115795  0.193675  0.478692  0.060985   \n",
       "6       0.965525  0.001901  0.005357  0.000000  0.006617  0.000000  0.000000   \n",
       "7       0.053409  0.040875  0.021348  0.005889  0.029946  0.007786  0.017647   \n",
       "8       0.099883  0.010456  0.001249  0.001508  0.001536  0.001626  0.017295   \n",
       "9       0.214486  0.035171  0.011197  0.004876  0.002900  0.001764  0.006285   \n",
       "10      0.039830  0.217681  0.065977  0.054700  0.055066  0.114603  0.030920   \n",
       "11      0.059643  0.148289  0.026262  0.021331  0.025223  0.018370  0.013977   \n",
       "12      0.041147  0.141635  0.026946  0.026690  0.013764  0.032837  0.019507   \n",
       "13      0.981387  0.007605  0.002779  0.000000  0.003058  0.000000  0.000000   \n",
       "14      0.378987  0.012357  0.001853  0.001519  0.000657  0.000205  0.003067   \n",
       "15      0.441030  0.015209  0.002739  0.002297  0.001082  0.000351  0.001911   \n",
       "16      0.053556  0.054183  0.025255  0.022874  0.025441  0.025023  0.015535   \n",
       "17      0.102517  0.229087  0.851452  1.000000  0.818792  1.000000  0.014379   \n",
       "18      0.068715  0.064639  0.004390  0.002662  0.003246  0.001796  0.010508   \n",
       "19      0.118993  0.107414  0.007693  0.006867  0.003728  0.003488  0.017748   \n",
       "20      0.030963  0.088403  0.012285  0.009234  0.014295  0.011787  0.023982   \n",
       "21      0.069271  0.167300  0.023523  0.020683  0.018408  0.016969  0.011011   \n",
       "22      0.059438  0.147338  0.013614  0.014994  0.006292  0.010027  0.009553   \n",
       "23      0.049810  0.081749  0.031135  0.028375  0.035400  0.047312  0.024485   \n",
       "24      0.010975  0.093156  0.008982  0.009117  0.009343  0.022174  0.044646   \n",
       "25      0.128300  0.047529  0.004632  0.003557  0.001684  0.001669  0.010005   \n",
       "26      0.171964  0.081749  0.020421  0.018634  0.015615  0.015083  0.012267   \n",
       "27      0.052795  0.134981  0.018528  0.015324  0.019118  0.017703  0.017044   \n",
       "28      0.095815  0.106464  0.030531  0.024994  0.029340  0.019889  0.011413   \n",
       "29      0.098186  0.095057  0.009747  0.006749  0.007255  0.005505  0.011564   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "131588  0.040942  0.046578  0.012768  0.009823  0.017981  0.017395  0.027401   \n",
       "131589  0.149312  0.075095  0.014138  0.013569  0.006550  0.006057  0.007290   \n",
       "131590  0.004243  0.383080  0.045475  0.042238  0.042299  0.184266  0.060633   \n",
       "131591  0.550776  0.024715  0.006807  0.005748  0.001592  0.000414  0.001659   \n",
       "131592  0.016242  0.188213  0.021589  0.019140  0.007989  0.030319  0.036099   \n",
       "131593  0.088089  0.056084  0.008781  0.008822  0.006827  0.007328  0.010709   \n",
       "131594  0.026251  0.303232  0.061385  0.060707  0.070317  0.133423  0.033233   \n",
       "131595  0.149576  0.060837  0.004108  0.004064  0.005291  0.002261  0.008346   \n",
       "131596  0.225724  0.032319  0.001410  0.001190  0.001085  0.000564  0.005480   \n",
       "131597  0.443518  0.003802  0.002457  0.001260  0.001203  0.000201  0.002413   \n",
       "131598  0.082997  0.097909  0.008136  0.008257  0.007194  0.007928  0.018552   \n",
       "131599  0.234387  0.046578  0.006727  0.005618  0.002952  0.002147  0.005480   \n",
       "131600  0.114194  0.225285  0.084666  0.064358  0.053252  0.056029  0.013022   \n",
       "131601  0.223910  0.058935  0.003867  0.003769  0.003889  0.001799  0.006385   \n",
       "131602  0.093620  0.062738  0.002900  0.003333  0.002498  0.002628  0.012368   \n",
       "131603  0.200029  0.026616  0.004471  0.003969  0.001919  0.001590  0.006435   \n",
       "131604  0.392918  0.035171  0.001651  0.000883  0.000727  0.000328  0.005380   \n",
       "131605  0.226807  0.038023  0.005800  0.003640  0.002646  0.001350  0.006234   \n",
       "131606  0.102517  0.229087  0.851452  1.000000  0.818792  1.000000  0.014379   \n",
       "131607  0.040503  0.157795  0.005196  0.003958  0.005678  0.004925  0.017949   \n",
       "131608  0.337372  0.043726  0.014380  0.008810  0.005943  0.002155  0.003871   \n",
       "131609  0.217091  0.032319  0.002256  0.001425  0.002193  0.000844  0.008698   \n",
       "131610  0.032251  0.047529  0.009063  0.012367  0.011928  0.015588  0.021217   \n",
       "131611  0.032572  0.230989  0.072945  0.063910  0.099382  0.080050  0.019708   \n",
       "131612  0.013872  0.067490  0.010191  0.010389  0.004146  0.011152  0.028457   \n",
       "131613  0.113813  0.083650  0.023120  0.022850  0.031567  0.021550  0.014832   \n",
       "131614  0.115862  0.074144  0.004068  0.003086  0.003240  0.001969  0.008999   \n",
       "131615  0.010653  0.268061  0.033552  0.028963  0.024896  0.050867  0.027501   \n",
       "131616  0.225110  0.024715  0.007572  0.003569  0.004657  0.000979  0.004173   \n",
       "131617  0.082587  0.129278  0.006565  0.006125  0.008688  0.004964  0.010960   \n",
       "\n",
       "               7  \n",
       "0       0.049130  \n",
       "1       0.045542  \n",
       "2       0.088174  \n",
       "3       0.038024  \n",
       "4       0.016037  \n",
       "5       0.054995  \n",
       "6       0.063093  \n",
       "7       0.071665  \n",
       "8       0.062857  \n",
       "9       0.013233  \n",
       "10      0.042642  \n",
       "11      0.049066  \n",
       "12      0.026090  \n",
       "13      0.056208  \n",
       "14      0.018120  \n",
       "15      0.020183  \n",
       "16      0.051461  \n",
       "17      0.049130  \n",
       "18      0.037777  \n",
       "19      0.024758  \n",
       "20      0.059441  \n",
       "21      0.039979  \n",
       "22      0.023609  \n",
       "23      0.058088  \n",
       "24      0.053136  \n",
       "25      0.018571  \n",
       "26      0.039066  \n",
       "27      0.052718  \n",
       "28      0.049098  \n",
       "29      0.038024  \n",
       "...          ...  \n",
       "131588  0.071944  \n",
       "131589  0.023663  \n",
       "131590  0.047519  \n",
       "131591  0.011944  \n",
       "131592  0.018904  \n",
       "131593  0.039721  \n",
       "131594  0.058518  \n",
       "131595  0.065789  \n",
       "131596  0.039313  \n",
       "131597  0.025027  \n",
       "131598  0.045166  \n",
       "131599  0.022417  \n",
       "131600  0.032127  \n",
       "131601  0.051375  \n",
       "131602  0.044006  \n",
       "131603  0.021923  \n",
       "131604  0.022481  \n",
       "131605  0.023298  \n",
       "131606  0.049130  \n",
       "131607  0.055832  \n",
       "131608  0.021117  \n",
       "131609  0.049678  \n",
       "131610  0.067240  \n",
       "131611  0.069603  \n",
       "131612  0.020784  \n",
       "131613  0.069753  \n",
       "131614  0.040687  \n",
       "131615  0.037905  \n",
       "131616  0.031418  \n",
       "131617  0.067605  \n",
       "\n",
       "[131618 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_trained = min_max_scaler.fit_transform(data_trained)\n",
    "data_tested=min_max_scaler.transform(data_tested)\n",
    "pd.DataFrame(data_tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing twitter columns from dataset\n",
    "data_trained=pd.DataFrame(data=data_trained)\n",
    "data_tested=pd.DataFrame(data=data_tested)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.31      0.37     19413\n",
      "         1.0       0.44      0.56      0.49     42870\n",
      "         2.0       0.39      0.32      0.35     36619\n",
      "         3.0       0.52      0.54      0.53     32716\n",
      "\n",
      "   micro avg       0.45      0.45      0.45    131618\n",
      "   macro avg       0.45      0.43      0.44    131618\n",
      "weighted avg       0.45      0.45      0.44    131618\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45088817638924766"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "decision = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "decision.fit(data_trained,y_train)\n",
    "decisionresult = decision.predict(data_tested)\n",
    "print(classification_report(y_test,decisionresult))\n",
    "sklearn.metrics.accuracy_score(y_test, decisionresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.02      0.04     19413\n",
      "         1.0       0.32      0.99      0.49     42870\n",
      "         2.0       0.00      0.00      0.00     36619\n",
      "         3.0       0.67      0.00      0.00     32716\n",
      "\n",
      "   micro avg       0.32      0.32      0.32    131618\n",
      "   macro avg       0.31      0.25      0.13    131618\n",
      "weighted avg       0.31      0.32      0.16    131618\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32406661702806605"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bernoulli Naive Bayes Classifier\n",
    "nbayes = BernoulliNB()\n",
    "nbayes.fit(data_trained,y_train.values.ravel())\n",
    "nbayesresult = nbayes.predict(data_tested)\n",
    "print(classification_report(y_test,nbayesresult))\n",
    "sklearn.metrics.accuracy_score(y_test, nbayesresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.29      0.36     19413\n",
      "         1.0       0.44      0.55      0.49     42870\n",
      "         2.0       0.39      0.32      0.35     36619\n",
      "         3.0       0.51      0.56      0.53     32716\n",
      "\n",
      "   micro avg       0.45      0.45      0.45    131618\n",
      "   macro avg       0.45      0.43      0.43    131618\n",
      "weighted avg       0.45      0.45      0.44    131618\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45111610873892627"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "random = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "random.fit(data_trained,y_train.values.ravel())\n",
    "randomresult = random.predict(data_tested)\n",
    "print(classification_report(y_test,randomresult))\n",
    "sklearn.metrics.accuracy_score(y_test, randomresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     19413\n",
      "         1.0       0.36      0.83      0.50     42870\n",
      "         2.0       0.32      0.06      0.11     36619\n",
      "         3.0       0.45      0.37      0.41     32716\n",
      "\n",
      "   micro avg       0.38      0.38      0.38    131618\n",
      "   macro avg       0.29      0.32      0.25    131618\n",
      "weighted avg       0.32      0.38      0.29    131618\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3791122794754517"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic regression\n",
    "lr = LogisticRegression(random_state = 0)\n",
    "lr.fit(data_trained, y_train.values.ravel())\n",
    "regres = lr.predict(data_tested)\n",
    "print(classification_report(y_test,regres))\n",
    "sklearn.metrics.accuracy_score(y_test, regres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.16      0.25        19\n",
      "         1.0       0.19      0.79      0.31        14\n",
      "         2.0       0.20      0.05      0.08        20\n",
      "         3.0       0.75      0.27      0.40        22\n",
      "\n",
      "   micro avg       0.28      0.28      0.28        75\n",
      "   macro avg       0.44      0.32      0.26        75\n",
      "weighted avg       0.46      0.28      0.26        75\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel = 'linear', random_state = 0)\n",
    "svm.fit(data_trained, y_train.values.ravel())\n",
    "svmres = svm.predict(data_tested)\n",
    "print(classification_report(y_test,svmres))\n",
    "sklearn.metrics.accuracy_score(y_test, svmres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.35      0.34     19413\n",
      "         1.0       0.42      0.47      0.44     42870\n",
      "         2.0       0.34      0.33      0.34     36619\n",
      "         3.0       0.50      0.43      0.46     32716\n",
      "\n",
      "   micro avg       0.40      0.40      0.40    131618\n",
      "   macro avg       0.40      0.39      0.40    131618\n",
      "weighted avg       0.41      0.40      0.40    131618\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40320472883648134"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "knn.fit(data_trained, y_train.values.ravel())\n",
    "knnres = knn.predict(data_tested)\n",
    "print(classification_report(y_test,knnres))\n",
    "sklearn.metrics.accuracy_score(y_test, knnres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
